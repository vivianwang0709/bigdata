# ************************************************************
# Sequel Pro SQL dump
# Version 4541
#
# http://www.sequelpro.com/
# https://github.com/sequelpro/sequelpro
#
# Host: 127.0.0.1 (MySQL 5.7.16)
# Database: bigdata
# Generation Time: 2017-04-17 08:24:58 +0000
# ************************************************************


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;


# Dump of table article
# ------------------------------------------------------------

LOCK TABLES `article` WRITE;
/*!40000 ALTER TABLE `article` DISABLE KEYS */;

INSERT INTO `article` (`pid`, `title`, `author`, `content`, `date`, `article_type`, `tag`, `view`, `url_from`, `pic_count`, `scontent`, `sort`)
VALUES
	(1,'數據解讀：曝光北京每平方房價成本','vivian','<article class=\"article-content\"><p style=\"text-align: center;\"><img alt=\"北京房價\" class=\"36img\" src=\"../static/pic/news/1_1.jpg\"/></p><p><strong>土地費用</strong>：按9000元/m2計算，另外加4%的契稅</p><p><strong>前期費用</strong>：約215~245元/m2</p><p><strong>建安費用</strong>：1600~1800元/m2。</p><p><strong>銷售費用</strong>：400元/m2</p><p><strong>間接費用、管理費</strong>：約50元/m2</p><p><strong>不可預見費</strong>：55~60元/m2</p><p><strong>貸款利息</strong>：按照年息15%計算，約375元/m2。</p><p>以上不含土地費合計約2695~2880元/m2。除此以外，還有所得稅(稅前利潤的33%)、土地增值稅(計算更複雜)等。</p><p>綜上所述，假設樓面地價9000元/m2，開發商的稅前成本約在12000元。再加上稅收，其它隱形成本，各位可以自己算算。</p><p>評論：房地產業真是暴利呢，政府的賣地模式更是暴利！</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/4442\">數據解讀：曝光北京每平方房價成本</a></p> </article>','2017-04-12','news',NULL,NULL,'http://www.36dsj.com/archives/4442',1,'以上不含土地費合計約2695~2880元/m2。除此以外，還有所得稅(稅前利潤的33%)、土地增值稅(計算更複雜)等。',NULL),
	(2,'數據解讀：一盒杜蕾斯的成本價格構成','vivian','<article class=\"article-content\"><p>按照杜蕾斯最新一季財報分析，以一盒零售價65元的18片裝杜蕾斯為例。其中原材料成本20.8元(32%)、經銷商分成13元(20%)、廣告投放7.8元(12%)、管理成本7.8元(12%)、純利潤5.85元(9%)、研發建設4.55(7%)、銷售費用3.25(5%)、營業稅1.95元(3%)。評：你能否大概算一下，從你第一次開始到現在，總共為杜蕾斯貢獻了多少錢？</p><p style=\"text-align: center;\"><img alt=\"杜蕾斯\" class=\"36img\" src=\"../static/pic/news/2_1.jpg\"/></p><p style=\"text-align: left;\">via：網易商業</p><p style=\"text-align: left;\">End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/3514\">數據解讀：一盒杜蕾斯的成本價格構成</a></p> </article>','2017-04-12','news',NULL,NULL,'http://www.36dsj.com/archives/3514',1,'按照杜蕾斯最新一季財報分析，以一盒零售價65元的18片裝杜蕾斯為例。其中原材料成本20.8元(32%)、經銷商分成13元(20%)、廣告投放7.8元(12%)、管理成本7.8元(12%)、純利潤5.85元(9%)、研發建設4.55(7%)、銷售費用3.25(5%)、營業稅1.95元(3%)。評：你能否大概算一下，從你第一次開始到現在，總共為杜蕾斯貢獻了多少錢？',NULL),
	(3,'見山只是山，見水只是水，數據分析禪宗三境界','vivian','<article class=\"article-content\"><p>最近經常聽到禪宗三境界，百度了一下典故出處：</p><p><em>佛教禪宗參禪三重境界，常被引申用來形容看待事物的不同境界。</em></p><p>佛教禪宗史書《五燈會元》，唐代禪宗大師青原惟信語：</p><p><em>老僧三十年前未參禪時，見山是山，見水是水。及即至後來，親見知識，有個入處；見山不是山，見水不是水。而今得個休歇處，依前見山只是山，見水只是水。</em></p><p><img alt=\"禪宗\" class=\"36img\" src=\"../static/pic/analysis/3_1.jpg\"/></p><p>這類高度抽象認識論，往往是可以放之四海而皆準的真（zhuang）知（bi）灼（li）見（qi）。回想一下這些年的工作經驗，在數據分析領域也是存在這三個階段</p><h2><strong>見山是山，見水是水</strong></h2><p>往往初入職場的數據新人都處於這個階段，這個階段的數據新人對業務瞭解不多，對數據的解讀往往只限於數據表現本身而不考慮其背後業務的意義，單純的解讀數據。</p><h2><strong>見山不是山，見水不是水</strong></h2><p>隨著對業務瞭解的加深，這個階段的分析師不僅僅關註數據，而是關註數據背後的業務意義。但是，其實一個數據可以從不同角度解讀，如果帶有太多的先入之見去解讀，也就失去了 數據分析知識發現的作用，或對數據進行過渡解讀，或者只是將數據作為驗證某種業務經驗的工具。</p><h2><strong>見山只是山，見水只是水</strong></h2><p>瞭解了先入之見的危害之後，就會再次回歸數據本身，對應的數據分析認識論就是敬畏數據，不過多的去解讀數據，而是應該大膽假設小心驗證。至此為止，也就完成了認識論的一個否定之否定的螺旋上升過程。</p><p>在現實的工作中，真正能夠達到第三階段的是鳳毛麟角。最佳的工作狀態還是業務人員與數據人員的組合，數據人員從數據角度給到專業分析，業務人員根據業務經驗與數據人員共同對分析結論進行修正。</p><p>via:Cloga的互聯網筆記</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/78975\">見山只是山，見水只是水，數據分析禪宗三境界</a></p> </article>','2017-04-12','analysis',NULL,NULL,'http://www.36dsj.com/archives/78975',1,'最近經常聽到禪宗三境界，百度了一下典故出處：',NULL),
	(4,'2016大數據創新大賽——機場客流量的時空分佈預測模型解析','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_1.jpg\"/></p><p>在大數據創新大賽上，來自浙江大學的SeaSide團隊帶來了關於機場客流量的時空分佈預測的解決方案。SeaSide團隊主要從時序模型、乘機流程、事件驅動、維度災難四個方面介紹了團隊的算法設計。</p><h2 id=\"1\"><b>背景介紹</b></h2><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_2.jpg\"/></p><p>SeaSide團隊所要解決的問題是利用機場大量的數據去預測每個時刻、每個無線AP的連接人數，這個結果可以很好的反映機場的客流時空分佈。可用的數據可以分為四個方面：</p><p><b>歷史連接：</b>包含每個無線AP的名稱、時間戳、連接人數；</p><p><b>地理位置：</b>包括登機口、無線AP所在的區域、無線AP的坐標、樓層、組號；</p><p><b>航班排班：</b>包括航班起降的排班信息、預計時間、實際時間；</p><p><b>旅客行為：</b>包括值機、安檢及離開。</p><h2 id=\"2\"><b>算法設計</b></h2><h4 id=\"3\"><b>時序模型</b></h4><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_3.jpg\"/></p><p>最初的想法是基於歷史值去預測當前值。上圖左邊是多天的真實值和三軸歷史均值，真實值一直圍繞歷史均值上下波動。三軸歷史均值在不同的情況下各有優缺點，把歷史均值擴展為更多的聚合指標之後，在聚合窗口上就得到了基礎模型。在計算聚合模型的過程中，有三種方式：同點、同時段、同組。最直接的方式就是計算同一AP點在同一歷史數據點的聚合值。為了減少時間維度的噪聲，可以計算同時段同一AP點的歷史數據聚合值。為了減少空間維度的噪聲，可以計算同組歷史數據聚合值。</p><h4 id=\"4\"><b>乘機流程</b></h4><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_4.jpg\"/></p><p>整個乘機的流程表明，其在值機、安檢、候機、行李提取處的旅客較為密集。所以，航班的起降會對這些區域造成很大的影響。</p><h4 id=\"5\"><b>事件驅動</b></h4><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_5.jpg\"/></p><p>比如，某個航班預計在某個時間起飛，可以從左上圖看到大家一般會提前一個半小時安檢，進入候機區等待。基於這些觀察統計了右邊的信息，預測時間點後面多個時間窗口上起飛航班的數量，根據這些信息可以得知每個時間段旅客起飛的人數。針對航班晚點情況，預測了滯留旅客人數。</p><p>除了起飛信息，SeaSide團隊還預測了到達信息（該時間點前/後到達航班數，對接機區、中轉區、取行李區影響比較大）、地理位置信息（區分特殊區域）以及時間信息（區分特殊時段，比如夜間、節假日）。</p><h4 id=\"6\"><b>維度災難</b></h4><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_6.jpg\"/></p><p>上述算法利用一個模型對所有的區域進行了預測，包含了五個方面的很多特征，然而，特征越多越好嗎？特征數量和預測效果的關係圖如左圖所示，過了臨界點之後，增加特征點會使預測效果變差。現在的模型有1472個特征，需要引入合理的先驗知識來有效降低維度。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_7.jpg\"/></p><p>首先，起飛降維方面，把機場分為四個區域，對於不同區域，並不是所有起飛登機口的信息都有用。比如，對於候機區，只需要為每個AP點獲取其最近的6個登機口。其餘區域的降維方法如上圖所示。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/analysis/4_8.jpg\"/></p><p>其次，對於時間特征，其實有序變量，與線性回歸不同，它是基於高維空間劃分的預測回歸模型，可以對有序變量根據訓練數據對其進行合理的區間劃分，因此，可以看做是1維的整數。對於位置特征，分區域之後只考慮本區域組號。</p><p><img alt=\"v\" class=\"36img\" src=\"../static/pic/analysis/4_9.jpg\"/></p><p>從左圖可以看出，多個無線AP在一段時間內連接數是趨於穩定的，所以在預測更短期的連接數的時候更加準確。</p><h2 id=\"7\"><b>總結展望</b></h2><p>模型的基礎是基於歷史連接數據的時序模型，加入事件驅動、降低維度進一步提升了模型。更精確的航班信息、坐標信息可以進一步提升算法。本算法可以提升網絡服務、機場服務的質量。</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/79548\">2016大數據創新大賽——機場客流量的時空分佈預測模型解析</a></p> </article>','2017-04-12','analysis',NULL,NULL,'http://www.36dsj.com/archives/79548',9,'在大數據創新大賽上，來自浙江大學的SeaSide團隊帶來了關於機場客流量的時空分佈預測的解決方案。SeaSide團隊主要從時序模型、乘機流程、事件驅動、維度災難四個方面介紹了團隊的算法設計。',NULL),
	(5,'用Spark學習矩陣分解推薦算法','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_1.jpg\"/></p><p>文 | 劉建平Pinard</p><p>在矩陣分解在協同過濾推薦算法中的應用中，我們對矩陣分解在推薦算法中的應用原理做了總結，這裡我們就從實踐的角度來用Spark學習矩陣分解推薦算法。</p><h2><strong>1. Spark推薦算法概述</strong></h2><p>在Spark MLlib中，推薦算法這塊只實現了基於矩陣分解的協同過濾推薦算法。而基於的算法是FunkSVD算法，即將m個用戶和n個物品對應的評分矩陣M分解為兩個低維的矩陣：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_2.jpg\"/></p><p>其中k為分解成低維的維數，一般遠比m和n小。如果大家對FunkSVD算法不熟悉，可以複習對應的原理篇。</p><h2><strong>2. Spark推薦算法類庫介紹</strong></h2><p>在Spark MLlib中，實現的FunkSVD算法支持Python,Java,Scala和R的接口。由於前面的實踐篇我們都是基於Python，本文的後面的介紹和使用也會使用MLlib的Python接口。</p><p>Spark MLlib推薦算法python對應的接口都在pyspark.mllib.recommendation包中，這個包有三個類，Rating, MatrixFactorizationModel和ALS。雖然裡面有三個類，但是算法只是FunkSVD算法。下麵介紹這三個類的用途。</p><p>Rating類比較簡單，僅僅只是為了封裝用戶，物品與評分這3個值。也就是說，Rating類裡面只有用戶，物品與評分三元組， 並沒有什麼函數接口。</p><p>ALS負責訓練我們的FunkSVD模型。之所以這兒用交替最小二乘法ALS表示，是因為Spark在FunkSVD的矩陣分解的目標函數優化時，使用的是ALS。ALS函數有兩個函數，一個是train,這個函數直接使用我們的評分矩陣來訓練數據，而另一個函數trainImplicit則稍微複雜一點，它使用隱式反饋數據來訓練模型，和train函數相比，它多了一個指定隱式反饋信心閾值的參數，比如我們可以將評分矩陣轉化為反饋數據矩陣，將對應的評分值根據一定的反饋原則轉化為信心權重值。由於隱式反饋原則一般要根據具體的問題和數據來定，本文後面只討論普通的評分矩陣分解。</p><p>MatrixFactorizationModel類是我們用ALS類訓練出來的模型，這個模型可以幫助我們做預測。常用的預測有某一用戶和某一物品對應的評分，某用戶最喜歡的N個物品，某物品可能會被最喜歡的N個用戶，所有用戶各自最喜歡的N物品，以及所有物品被最喜歡的N個用戶。</p><p>對於這些類的用法我們再後面會有例子講解。</p><h2><strong>3. Spark推薦算法重要類參數</strong></h2><p>這裡我們再對ALS訓練模型時的重要參數做一個總結。</p><p><strong>1) ratings :</strong> 評分矩陣對應的RDD。需要我們輸入。如果是隱式反饋，則是評分矩陣對應的隱式反饋矩陣。</p><p><strong>2) rank :</strong> 矩陣分解時對應的低維的維數。即PTm×kQk×nPm×kTQk×n中的維度k。這個值會影響矩陣分解的性能，越大則算法運行的時間和占用的內存可能會越多。通常需要進行調參，一般可以取10-200之間的數。</p><p><strong>3) iterations :</strong>在矩陣分解用交替最小二乘法求解時，進行迭代的最大次數。這個值取決於評分矩陣的維度，以及評分矩陣的繫數程度。一般來說，不需要太大，比如5-20次即可。默認值是5。</p><p><strong>4) lambda:</strong> 在 python接口中使用的是lambda_,原因是lambda是Python的保留字。這個值即為FunkSVD分解時對應的正則化繫數。主要用於控制模型的擬合程度，增強模型泛化能力。取值越大，則正則化懲罰越強。大型推薦系統一般需要調參得到合適的值。</p><p><strong>5) alpha :</strong> 這個參數僅僅在使用隱式反饋trainImplicit時有用。指定了隱式反饋信心閾值，這個值越大則越認為用戶和他沒有評分的物品之間沒有關聯。一般需要調參得到合適值。</p><p>從上面的描述可以看出，使用ALS算法還是蠻簡單的，需要註意調參的參數主要的是矩陣分解的維數rank, 正則化超參數lambda。如果是隱式反饋，還需要調參隱式反饋信心閾值alpha 。</p><h2><strong>4. Spark推薦算法實例</strong></h2><p>下麵我們用一個具體的例子來講述Spark矩陣分解推薦算法的使用。</p><p>這裡我們使用MovieLens 100K的數據，<a href=\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\" target=\"_blank\">數據下載鏈接在這</a>。</p><p>將數據解壓後，我們只使用其中的u.data文件中的評分數據。這個數據集每行有4列，分別對應用戶ID，物品ID，評分和時間戳。由於我的機器比較破，在下麵的例子中，我只使用了前100條數據。因此如果你使用了所有的數據，後面的預測結果會與我的不同。</p><p>首先需要要確保你安裝好了Hadoop和Spark(版本不小於1.6)，並設置好了環境變量。一般我們都是在ipython notebook(jupyter notebook)中學習，所以最好把基於notebook的Spark環境搭好。當然不搭notebook的Spark環境也沒有關係，只是每次需要在運行前設置環境變量。</p><p>如果你沒有搭notebook的Spark環境，則需要先跑下麵這段代碼。當然，如果你已經搭好了，則下麵這段代碼不用跑了。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_3.jpg\"/></p><p>在跑算法之前，建議輸出Spark Context如下，如果可以正常打印內存地址，則說明Spark的運行環境搞定了。</p><p>print sc</p><p>比如我的輸出是：</p><p>首先我們將u.data文件讀入內存，並嘗試輸出第一行的數據來檢驗是否成功讀入，註意複製代碼的時候，數據的目錄要用你自己的u.data的目錄。代碼如下：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_4.jpg\"/></p><p>輸出如下：</p><p>u’196t242t3t881250949′</p><p>可以看到數據是用t分開的，我們需要將每行的字符串劃開，成為數組，並只取前三列，不要時間戳那一列。代碼如下：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_5.jpg\"/></p><p>輸出如下：</p><p>[u’196′, u’242′, u’3′]</p><p>此時雖然我們已經得到了評分矩陣數組對應的RDD，但是這些數據都還是字符串，Spark需要的是若干Rating類對應的數組。因此我們現在將RDD的數據類型做轉化，代碼如下：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_6.jpg\"/></p><p>輸出如下：</p><p>Rating(user=196, product=242, rating=3.0)</p><p>可見我們的數據已經是基於Rating類的RDD了，現在我們終於可以把整理好的數據拿來訓練了，代碼如下, 我們將矩陣分解的維度設置為20，最大迭代次數設置為5，而正則化繫數設置為0.02。在實際應用中，我們需要通過交叉驗證來選擇合適的矩陣分解維度與正則化繫數。這裡我們由於是實例，就簡化了。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/5_7.jpg\"/></p><p>將模型訓練完畢後，我們終於可以來做推薦系統的預測了。</p><p>首先做一個最簡單的預測，比如預測用戶38對物品20的評分。代碼如下：</p><p>print model.predict(38,20)</p><p>輸出如下：</p><p>0.311633491603</p><p>可見評分並不高。</p><p>現在我們來預測了用戶38最喜歡的10個物品，代碼如下：</p><p>print model.recommendProducts(38,10)</p><p>輸出如下：</p><p>[  Rating(user=38, product=95, rating=4.995227969811873),</p><p>Rating(user=38, product=304, rating=2.5159673379104484),</p><p>Rating(user=38, product=1014, rating=2.165428673820349),</p><p>Rating(user=38, product=322, rating=1.7002266119079879),</p><p>Rating(user=38, product=111, rating=1.2057528774266673),</p><p>Rating(user=38, product=196, rating=1.0612630766055788),</p><p>Rating(user=38, product=23, rating=1.0590775012913558),</p><p>Rating(user=38, product=327, rating=1.0335651317559753),</p><p>Rating(user=38, product=98, rating=0.9677333686628911),</p><p>Rating(user=38, product=181, rating=0.8536682271006641)]</p><p>可以看出用戶38可能喜歡的對應評分從高到低的10個物品。</p><p>接著我們來預測下物品20可能最值得推薦的10個用戶，代碼如下：</p><p>print model.recommendUsers(20,10)</p><p>輸出如下：</p><p>[  Rating(user=115, product=20, rating=2.9892138653406635),</p><p>Rating(user=25, product=20, rating=1.7558472892444517),</p><p>Rating(user=7, product=20, rating=1.523935609195585),</p><p>Rating(user=286, product=20, rating=1.3746309116764184),</p><p>Rating(user=222, product=20, rating=1.313891405211581),</p><p>Rating(user=135, product=20, rating=1.254412853860262),</p><p>Rating(user=186, product=20, rating=1.2194811581542384),</p><p>Rating(user=72, product=20, rating=1.1651855319930426),</p><p>Rating(user=241, product=20, rating=1.0863391992741023),</p><p>Rating(user=160, product=20, rating=1.072353288848142)]</p><p>現在我們來看看每個用戶最值得推薦的三個物品，代碼如下:</p><p>print model.recommendProductsForUsers(3).collect()</p><p>由於輸出非常長，這裡就不將輸出copy過來了。</p><p>而每個物品最值得被推薦的三個用戶，代碼如下：</p><p>print model.recommendUsersForProducts(3).collect()</p><p>同樣由於輸出非常長，這裡就不將輸出copy過來了。</p><p>希望上面的例子對大家使用Spark矩陣分解推薦算法有幫助。</p><p>via:劉建平Pinard博客</p><p>推薦算法更多內容：</p><p><a href=\"http://www.36dsj.com/archives/62617\" title=\"推薦系統常用的推薦算法 | 36大數據\">推薦系統常用的推薦算法</a></p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/76237\">用Spark學習矩陣分解推薦算法</a></p> </article>','2017-04-12','code',NULL,NULL,'http://www.36dsj.com/archives/76237',7,'在矩陣分解在協同過濾推薦算法中的應用中，我們對矩陣分解在推薦算法中的應用原理做了總結，這裡我們就從實踐的角度來用Spark學習矩陣分解推薦算法。',NULL),
	(6,'用Spark分析Amazon的8000萬商品評價(內含數據集、代碼、論文)','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_1.jpg\"/></p><p>文 | Max Woolf</p><p>儘管數據科學家經常通過分佈式雲計算來處理數據，但是即使在一般的筆記本電腦上，只要給出足夠的內存，Spark也可以工作正常(在這篇文章中，我使用2016年MacBook Pro / 16GB內存，分配給Spark 8GB內存)。</p><p>此外，通過Maxcompute及其配套產品，低廉的大數據分析僅需幾步。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_2.jpg\"/></p><p>亞馬遜的商品評論和評分是一個非常重要的業務。 亞馬遜上的客戶經常基於這些評論做出購買決定，並且單個不良評論可以導致潛在購買者重新考慮。 幾年前，我寫了一篇非常受歡迎的博客文章，題為“120萬亞馬遜評論統計分析“。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_3.jpg\"/></p><p>當時，我只限於1200萬評論，因為嘗試處理更多的數據會導致內存不足，以至於我的R語言代碼需要運行幾個小時。</p><p>Apache Spark是一個高效的開源大數據計算框架，在過去幾年中已經非常流行(對於使用Spark和Python的好教程，我推薦免費的eDX課程)。儘管數據科學家經常通過分佈式雲計算來處理數據，但是即使在一般的筆記本電腦上，只要給出足夠的內存，Spark也可以工作正常(在這篇文章中，我使用2016年MacBook Pro / 16GB內存，分配給Spark 8GB內存)。</p><p>我寫了一個簡單的Python腳本，用來合併Julian McAuley、Rahul Pandey和Jure Leskovecucehua在2015年發佈“Inferring Networks of Substitutable and Complementary Products”論文時準備的亞馬遜產品評論數據集中每個類別的評級數據 。成果是一個4.53 GB的CSV，肯定不能在Microsoft Excel中打開。選取和整合的數據集包括：留下評論的用戶的用戶名，指明是哪一個接收評論亞馬遜產品的id，從1到5的用戶給出的評級，以及評論寫入的時間(精確到天)。 我們還可以從數據子集的名稱推斷已評價產品的類別。</p><p>然後，使用面對R語言的新的升級包，我可以使用一個spark_connect()命令輕鬆啟動本地Spark集群，並使用單個spark_read_csv()命令很快將整個CSV加載到集群中。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_4.jpg\"/></p><p>在數據集中總共有8074萬條記錄，即8.074e + 07條。如果使用傳統工具(如dplyr或甚至Python pandas)高級查詢，這樣的數據集將需要相當長的時間來執行。</p><p>使用sparklyr，操作實際很大的數據就像對只有少數記錄的數據集執行分析一樣簡單(並且比上面提到的eDX類中教授的Python方法簡單一個數量級)。</p><h2><strong>試探性分析</strong></h2><p>(您可以查看用於Spark處理數據的R代碼，併在此R Notebook中生成可視化數據)有20,368,412個有效id的用戶在此數據集中提供評論。 其中51.9%的用戶只寫了一篇評論。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_5.jpg\"/></p><p>相應地，此數據集中有8,210,439個單獨的產品，其中43.3%只有一個評論。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_6.jpg\"/></p><p>刪除幾個重覆的評分後，我為每個評分添加了幾個函數，這可能有助於說明審核行為隨時間的變化：一個能表示給定該評論的作者的#評論排名值(作者的第一次評論，第二次評論等)，一個指示給定接到該評論的產品已經接收到的#評論(產品的第一評論，產品的第二評論等)的評級值以及進行評論的月份和年份。</p><p>前兩個添加的函數需要非常大的處理能力，這突出Spark的性能事實上，Spark使用默認情況下所有的CPU核心，而典型的R / Python方法是單線程的!)</p><p>這些更改被緩存到Spark DataFrame df_t中。 如果我想確定哪個亞馬遜產品類別獲得最佳平均評論評分，我可以按類別整合數據，計算每個類別的平均評分，然後排序。多虧Spark的強大功能，這個數百萬記錄的數據處理需要幾秒鐘。</p><p> </p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_7.jpg\"/></p><p>也可以使用ggplot2以圖表形式顯示：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_8.jpg\"/></p><p>數字音樂/ CD產品平均獲得最高評價，而視頻游戲和手機得到最低平均評價，評分範圍為0.77。 這確實說明瞭一些直觀的聯繫; 購買數字音樂和CD這類產品時，你知道你會得到什麼，沒有產生隨機缺陷機會，而手機和配件根據背後的第三方賣家的會有不同的質量(電子游戲尤其容易由於微小的不合理而產生評論的“爆炸”)。</p><p>我們可以將每個條細分分成從1-5的每個評級的百分比，更利於該可視化。 也可以將餅圖圖表劃分成不同類別，但像這樣碼成條形圖再縮放到100%能看起來更清爽。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_9.jpg\"/></p><p>新的圖表確實有助於支持上述理論; 頂部的類別的4/5星評級的百分比顯著高於底部類別，並且1/2/3星級評分的比例低得多，底部類別與之相反。那麼這些故障如何隨時間而改變? 還有其他因素在發揮嗎?</p><h2><strong>隨時間變化的評級</strong></h2><p>也許出現在二十世紀二十年代社會媒體中的二元評級“喜歡/不喜歡”已經轉化為五星級評論系統的行為。 以下是從2000年1月至2014年7月每月撰寫的評論的評分細目：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_10.jpg\"/></p><p>投票行為在一段時間內非常輕微地振蕩，沒有清晰的尖峰或拐點，這與該理論衝突。</p><h2><strong>平均值分佈</strong></h2><p>我們應該看看亞馬遜的產品分數的全球平均值(即客戶在購買產品時看到的)，以及給出分級的用戶。在我們期望中兩者分佈匹配，所以任何偏差都會很有趣。關註至少評級5的產品時，有4.16平均總評級：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_11.jpg\"/></p><p>當查看反應用戶給出的總體評分類似的圖表時(5個評級最低)，平均評級略高於4.20。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_12.jpg\"/></p><p>這兩種分配的主要區別是亞馬遜客戶只有5星評價的比例明顯更高。歸納和總結兩個圖表可以清楚突出了差異。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_13.jpg\"/></p><h2><strong>特別的評論</strong></h2><p>幾個帖子前，我討論了Reddit帖子的第一個評論為何比以後的評論有更大的影響。 在做出越來越多的評論後，用戶評分行為是否會改變? 同一件產品的第一次評價，與典型的評級行為是否不同?這裡是某個用戶給出的幾個亞馬遜評論的評分細目：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_14.jpg\"/></p><p>第一個用戶評論的評分比之後的評價稍高。其他情況下，評級行為大部分是相同的，雖然用戶給4星而不是5星評價的比例增加，由於這樣更舒適。相比之下，這裡是某亞馬遜產品收到的幾個評論的評分細目：</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/6_15.jpg\"/></p><p>第一個產品評論是5星評價的可能略高於隨後的評論。 然而，在第10次審查之後，評級分佈沒有變化，這意味著特殊評級行為獨立於該閾值之後的當前評分。</p><h2><strong>總結</strong></h2><p>的確，這篇博文中使用數據多於分析它。 在未來技術發佈中，可能更有趣的是特定條件下的行為，例如根據該產品/該用戶以前的評價，預測評論的評級。 然而，這篇文章表明，雖然“大數據”可能現在仍是一個令人費解的流行語，但即使你不必為一家財富500強公司工作，也能夠理解它。 即使數據集由5個簡單的函數組成，您也可以歸納大量的結論。</p><p>而這篇文章甚至不需要查看亞馬遜的產品評論的文本或與產品相關的元數據! 只要有想法，就能完成。</p><p>您可以在<a href=\"http://minimaxir.com/notebooks/amazon-spark/?spm=5176.100239.blogcont69165.12.tp1uzQ\" target=\"_blank\">R Notebook</a>中查看所有用於可視化Amazon數據的R和ggplot2代碼。您還可以在此GitHub存儲庫中查看用於此帖子的<a href=\"https://github.com/minimaxir/amazon-spark?spm=5176.100239.blogcont69165.13.02Gxvd\" target=\"_blank\">鏡像/數據。</a></p><p><a href=\"http://minimaxir.com/2017/01/amazon-spark/?spm=5176.100239.blogcont69165.14.02Gxvd&amp;winzoom=1\" target=\"_blank\">原文鏈接&gt;&gt;&gt;</a></p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/76717\">用Spark分析Amazon的8000萬商品評價(內含數據集、代碼、論文)</a></p> </article>','2017-04-12','code',NULL,NULL,'http://www.36dsj.com/archives/76717',15,'儘管數據科學家經常通過分佈式雲計算來處理數據，但是即使在一般的筆記本電腦上，只要給出足夠的內存，Spark也可以工作正常(在這篇文章中，我使用2016年MacBook Pro / 16GB內存，分配給Spark 8GB內存)。',NULL),
	(7,'微博終結者爬蟲','vivian','<article class=\"article-content\"><p><img alt=\"爬蟲\" class=\"36img\" src=\"../static/pic/code/7_1.jpg\"/></p><h2><strong>微博終結者爬蟲</strong></h2><p>關於聊天對話系統我後面會開源一個項目，這個repo目的是基於微博構建一個高質量的對話語料，本項目將繼續更進開發，大家快star！！永遠開源！</p><p>這個項目致力於對抗微博的反爬蟲機制，集合眾人的力量把微博成千上萬的微博評論語料爬取下來並製作成一個開源的高質量中文對話語料，推動中文對話系統的研發。 本系統現已實現：</p><ul><li>爬取指定id用戶的微博數，關註數，粉絲數，所有微博內容以及所有微博對應的評論；</li><li>作者考慮到製作對話系統的可行性以及微博語料的難處理性，爬取過程中，所有微博會保存為可提取的形式，具體可以參照爬取結果保存樣例；</li><li>本項目不依賴於任何第三方爬取框架，但手動實現了一個多線程庫，當爬取多用戶時會開啟上百條線程工作，爬取速度在每小時百萬級別；</li><li>本項目最終目的是為了充分利用龐大的微博平臺構建一個開源高質量的中文對話系統（據作者所知，很多公司對自己的數據視如珍寶，鄙之）；</li><li>除此之外，本項目還可以用於指定用戶評論分析，比如爬取羅永浩的微博可以分析他第二年鎚子手機的銷量（牛逼把）</li></ul><p>希望更多童鞋們contribute進來，還有很多工作要做，歡迎提交PR！</p><h2><strong>為人工智能而生</strong></h2><p>中文語料一直以來備受詬病，沒有機構或者組織去建立一些公開的數據集，反觀國外，英文語料相當豐富，而且已經做的非常精準。</p><p><img alt=\"爬蟲\" class=\"36img\" src=\"../static/pic/code/7_2.jpg\"/></p><p>微博語料作者認為是覆蓋最廣，最活躍最新鮮的語料，使用之構建對話系統不說模型是否精準，但新鮮的詞彙量是肯定有的。</p><h2><strong>爬取結果</strong></h2><p>指定用戶的微博和評論形式如下：</p><pre class=\"prettyprint hljs markdown\">E<br>4月15日#傲嬌與偏見# 超前點映，跟我一起去搶光它 [太開心]  傲嬌與偏見 8.8元超前點映  順便預告一下，本周四（13號）下<br>午我會微博直播送福利，不見不散哦[壞笑]   電影傲嬌與偏見的秒拍視頻 <span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">200b</span>&gt;</span></span><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">200b</span>&gt;</span></span><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">200b</span>&gt;</span></span><br>E<br>F<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">哈哈哈哈哈哈狗-</span>&gt;</span></span>: 還唱嗎[doge]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">緑麓</span>&gt;</span></span>: 綠麓！<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">哈哈哈哈哈哈狗-</span>&gt;</span></span>: [<span class=\"hljs-string\">doge</span>][<span class=\"hljs-symbol\">doge</span>]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 哈哈哈哈哈哈哈<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">五隻熱巴肩上扛</span>&gt;</span></span>: 大哥已經唱完了[哆啦A夢吃驚]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">哈哈哈哈哈哈狗-</span>&gt;</span></span>: 大哥[哆啦A夢吃驚]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">獨愛Dear</span>&gt;</span></span>: 10:49坐等我迪的直播[<span class=\"hljs-string\">喵喵</span>][<span class=\"hljs-symbol\">喵喵</span>][<span class=\"hljs-string\">喵喵</span>]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: 對不起[可憐]我不趕<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: 哈狗[<span class=\"hljs-string\">哆啦A夢花心</span>][<span class=\"hljs-symbol\">哆啦A夢花心</span>]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 哈狗來了 哈哈哈<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: [攤手]綠林鹿去哪裡了！！！！<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">哈哈哈哈哈哈狗-</span>&gt;</span></span>: 阿健[哆啦A夢花心]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 然而你還要趕我出去[喵喵]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: 我也很絕望<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 只剩翻牆而來的我了<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: [攤手]我能怎麼辦<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: [攤手]一首歌唱到一半被掐斷是一個歌手的恥辱[攤手]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 下一首<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">四隻熱巴肩上扛</span>&gt;</span></span>: 最害怕就是黑屋[攤手]<br><span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">至誠dliraba</span>&gt;</span></span>: 我腦海一直是 跨過傲嬌與偏見 永恆的信念<br>F</pre><p>說明：</p><ul><li>E E 表示微博內容的開頭和結果</li><li>F F表示所有評論的開頭和結尾</li><li>每條評論中 &lt;&gt; 是發起評論的用戶id， $$ 中是at用戶的id</li></ul><h2><strong>Future Work</strong></h2><p>現在爬取的語料是最原始版本，大家對於語料的用途可以從這裡開始，可以用來做話題評論機器人，但作者後面將繼續開發後期處理程序，把微博raw data變成對話形式，並開源。 當然也歡迎有興趣的童鞋們給我提交PR，選取一個最佳方案，推動本項目的進展。</p><h2><strong>Contact</strong></h2><p>對於項目有任何疑問的可以聯繫我 wechat: <code>jintianiloveu</code> , 也歡迎提issue</p><h2><strong>Copyright</strong></h2><p>(c) 2017 Jin Fagang &amp; Tianmu Inc. LICENSE Apache 2.0</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/80454\">微博終結者爬蟲</a></p> </article>','2017-04-17','code',NULL,NULL,'http://www.36dsj.com/archives/80454',2,'關於聊天對話系統我後面會開源一個項目，這個repo目的是基於微博構建一個高質量的對話語料，本項目將繼續更進開發，大家快star！！永遠開源！',NULL),
	(8,'【強烈推薦】十三個鮮為人知的大數據學習網站','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_1.jpg\"/></p><p>文|鄧凱</p><p><strong>數據分析重要性</strong></p><p>越來越多的管理者意識到數據分析對經濟發展、企業運營的重要意義。</p><p>在古代，得琅琊閣者得天下 現在，得大數據者得天下。</p><p><strong>我總結的數據分析五步走：</strong></p><p>1、鎖定分析目標，梳理思路，叫紙上談兵；</p><p>2、把雜亂的數據整理出圖表報表，用數據探業務，叫自問數答；</p><p>3、鎖定核心抓重點，設定最終算法，叫挾天子以令諸侯；</p><p>4、梳理重點發現，準備劇本開拍，接受PK，叫才辨無雙；</p><p>5、效果梳理，總結經驗，叫內視反聽。</p><p>知道了數據的重要性，也瞭解了數據分析的步驟，那麼如何更好的學習並運用呢</p><p><strong>現在學習的途徑很多，數據君整理幾個大家不知道的網站，讓你開開眼界：</strong></p><p><strong>都是國外的學習站點，所以有時候打不開，原因你懂的</strong></p><h2><strong>一、如何用R的處理大數據</strong></h2><p><a href=\"http://www.xmind.net/m/LKF2/\" target=\"_blank\">http://www.xmind.net/m/LKF2/</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_2.jpg\"/></p><h2><strong>二、R語言的工具包</strong></h2><p><a href=\"https://cran.r-project.org/web/views/\" target=\"_blank\">https://cran.r-project.org/web/views/</a></p><p>裡面含機器學習，自然語言處理，時間序列分析，空間信息分析，多重變量分析，計量經濟學，心理統計學，社會學統計，化學計量學，藥物代謝動力 等</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_3.jpg\"/></p><h2><strong>三、幫你獲得python大數據處理工具大全</strong></h2><p><a href=\"http://www.xmind.net/m/WvfC\" target=\"_blank\">http://www.xmind.net/m/WvfC</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_4.jpg\"/></p><h2><strong>四、學習Python語言的，個人強烈推薦</strong></h2><p><a href=\"https://learnpythonthehardway.org/book/\" target=\"_blank\">https://learnpythonthehardway.org/book/</a></p><p>說實話Python最近今年太火了，靜下心學這個語言沒有錯！</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_5.jpg\"/></p><h2><strong>五、SAS圖例集</strong></h2><p><a href=\"http://robslink.com/SAS/Home.htm\" target=\"_blank\">http://robslink.com/SAS/Home.htm</a></p><p>用SAS也可以做出很漂亮的圖形，這裡就要提到一位大牛：Robert Allison。在他的網站上給出了非常全面的SAS圖例和相應的實現代碼</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_6.jpg\"/></p><h2><strong>六、美國布朗大學概率和統計的可視化導論，一個非常棒的可視化概率及統計的學習網站</strong></h2><p><a href=\"http://students.brown.edu/seeing-theory/?vt=4\" target=\"_blank\">http://students.brown.edu/seeing-theory/?vt=4</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_7.jpg\"/></p><h2><strong>七、教你玩動態的GIF圖表</strong></h2><p><a href=\"http://lenagroeger.com/\" target=\"_blank\">http://lenagroeger.com/</a></p><p>教你如何把手中的數據變成炫酷的GIF動圖？ 這個網站有各種類型的GIF圖</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_8.jpg\"/></p><h2><strong>八、如何選擇機器學習算法</strong></h2><p><a href=\"http://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/\" target=\"_blank\">http://blogs.sas.com/content/subconsciousmusings/2017/04/12/machine-learning-algorithm-use/</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_9.jpg\"/></p><h2><strong>九、一套數據，25種可視化</strong></h2><p><a href=\"http://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways\" target=\"_blank\">http://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways</a></p><p>仔細看了一下受益匪淺，同一組數據做出來的效果不同，看數的角度也不同</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_10.jpg\"/></p><h2><strong>十、大數據數據處理資源</strong></h2><p><a href=\"http://usefulstuff.io/big-data/\" target=\"_blank\">http://usefulstuff.io/big-data/</a></p><p>從框架、分佈式編程、分佈式文件系統、鍵值數據模型、圖數據模型、數據可視化、列存儲、機器學習等</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_11.jpg\"/></p><h2><strong>十一、裡面蘊含編程語言、機器算法、大數據等，內容巨豐富</strong></h2><p><a href=\"https://www.analyticsvidhya.com/blog/2017/02/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data/\" target=\"_blank\">https://www.analyticsvidhya.com/blog/2017/02/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data/</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_12.jpg\"/></p><h2><strong>十二、推薦排名前50個開源的Web爬蟲</strong></h2><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_13.jpg\"/></p><h2><strong>十三、學習數據挖掘、機器學習的好網站，寫的很全面很系統，適合各個級別的高手</strong></h2><p><a href=\"https://www.autonlab.org/tutorials\" target=\"_blank\">https://www.autonlab.org/tutorials</a></p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/code/8_14.jpg\"/></p><p>via：datakong</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/80382\">【強烈推薦】十三個鮮為人知的大數據學習網站</a></p> </article>','2017-04-17','code',NULL,NULL,'http://www.36dsj.com/archives/80382',14,'越來越多的管理者意識到數據分析對經濟發展、企業運營的重要意義。',NULL),
	(9,'10分鐘讀懂Uber的動態定價策略','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/news/9_1.jpg\"/></p><h2><strong>一、什麼是動態定價策略</strong></h2><p>本文所指的動態定價策略僅僅限定在共享經濟中的討論，其他類型中的暫不涵蓋。</p><p>在 Uber 提出並使用動態定價策略後，經過了這段時間的市場驗證，我們再回過頭來看看動態定價策略的提出，發展，利弊以及應用。</p><p>動態定價策略並沒有一個嚴格的定義，但是他提現了一個經濟學中的核心概念就是：供需平衡。</p><p>因此我給動態定價策略的一個簡單定義是：在一定的市場環境中，供需雙方為達到平衡點而做出的價格調整。</p><p>動態定價並不是新概念，加上了算法，智能，大數據等一堆詞以後，顯得有些驚艷了而已，動態定價在我們日常生活中的使用非常廣泛，而且影響著每個人。</p><p>舉一個很簡單的例子，過年的時候，蔬菜普遍貴了，原因就是供應少了，所以蔬菜的價格上升，只是在互聯網情況下，這種變動會更加快，更加敏捷。</p><h2><strong>二、為什麼使用動態定價策略</strong></h2><p>任何一個方法或者技術的產生，都是為瞭解決某一個問題，尤其是我們在做產品的時候，遇到一個問題，要思考解決這個問題的解決辦法，提出一二三，從中選取最優的方案。</p><p>從 Uber 的角度要解決的問題是在高峰或者異常天氣的情況下，Uber 的司機少，乘客打不到車的問題，為瞭解決這個問題，我們可以沙盤推演一下，Uber 能想到的解決辦法：</p><ol><li>自由市場，置之不顧</li><li>自己購買一些車並雇佣一些司機來應對</li><li>通過與個人司機或其他交通部門簽訂協議，由第三方提供運力來應對</li><li>通過一定的調度策略，來趨勢自己平臺上的司機來應對</li></ol><p>第一種情況是完全自由市場，沒有任何宏觀調控的時候的樣子，我們認為 Uber 是一家理性的公司，顯然 Uber 不會這麼做</p><p>第二種情況 Uber 幾乎也不會採取，因為從 Uber 的一開始就定位於共享經濟，C2C，所以一旦踏入自己購買就將是一個 B2C 和 C2C 混合的經濟體，這是 Uber 不願意考慮的方向。</p><p>第三種情況與第二種情況也類似，都會加重公司的負擔並且與初始定位違背，所以也被否決。</p><p><span style=\"color: #ff6600;\"><strong>那就只剩下第四種情況，調度。</strong></span></p><p>這是一個很神奇的詞，通過某種方法，可以讓原來同樣的東西產生更大的生產力，我們此處可以對第四種情況再細分成以下幾個選項：</p><p>當遇上高峰或打不到車的時候，通知打車人現在沒車，請選擇其他交通工具或繼續等待</p><p>當遇上高峰或打不到車的時候，通知司機師傅，現在某某地有很多人要搭車，趕緊過去拉人，去我就獎勵你，不去我就懲罰你</p><p>當遇上高峰或打不到車的時候，通知打車人現在沒車，你可以加點錢，“或許”就有人來拉你了</p><p>第一點並沒有解決這個問題，雖然可以作為一種決絕方案，但只是陳述了一個事實，而且對自己用戶並沒有負責人，所以基本可以不採取。</p><p>第二點相信是很多產品的選擇，畢竟在威逼利誘下還是會有用戶選擇去做這件事，但是這裡也會產生兩個問題：</p><p>如果是懲罰將會產生較差的體驗和叛逆，如果是補貼獎勵則會加大企業的支出，也就是開啟燒錢模式的大招。</p><p>Uber 完全有理由可以選擇這一方案，但是我們看到了 Uber 選擇了第三種方案，打不到車的用戶請多出點錢，也就是動態定價的基本原型。</p><p>至於這種方案是否是合理的，又有什麼利弊，我們在第四點裡面討論。</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/news/9_2.jpg\"/></p><h2><strong>三、動態定價策略的經濟學基礎</strong></h2><p>綜上我們基本確定了選擇動態變動價格的策略是基本可行的，正如我們前面所說的，這種策略由來已久，從經濟學的角度我們來分析一下他的可行性。</p><p>首先經濟學十大原理之一：人們對激勵做出反應，這應該是動態定價策略的理論基礎。</p><p>我們在一個簡單的模型下麵，我們來看一下整個的變動：</p><p>黃線：表示接單人(供給)，也就司機師傅，價格越高，接單人越多</p><p>綠線：表示發單人(需求)，也就是打車人，價格越低，打車人越多</p><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/news/9_3.jpg\"/></p><p>現在我們做以下的假設：</p><p>1. 假設從 M1 地到 M2 地點的正常價格為 40 元，在這種情況下，剛好達到A的均衡點，所有的人都可以打到車，所有的車也剛好都拉到人</p><p>2. 現在假設遇到了糟糕的天氣，司機師傅變少了，黃色線上移變動為黃色的虛線，打車人並沒有變，於是產生了：價格上漲，部分打車人打到了車，而部分打車人因為價格上漲而退出了打車的行列，這時達到了 A1 的平衡點</p><p>3. 現在情況更複雜一些，假設遇到節假日，打車的人突然增多，而司機並沒有增加，這時綠線上移，因為加價而讓更多的司機加入進來，從而達到了新平衡點 A2</p><p>4. 現在分析最複雜的情況，2，3 點同時產生，即司機減少，而打車人增加，這時兩條虛線相交點，便是新的平衡點。</p><p>即價格的上漲促使產生了兩個結果：</p><ol><li>更多的司機加入進來</li><li>部分打車的人因為價格問題選擇其他的方案</li></ol><p>這是符合經濟學的核心原理：供需。</p><p>在此基礎上，當我們再加入平臺補貼，抽傭以及其他一些獎勵的制度的時候，曲線將變得更加複雜，並且會產生無效的損耗，但是目的總是在調節供需雙方達到一個合理的平衡點。</p><h2><strong>四、動態定價策略的利弊</strong></h2><p>在有了理論支持以後，我們再回過頭來看看這種策略的利弊。</p><p><span style=\"color: #ff6600;\"><strong>1、優點</strong></span></p><ol><li>平臺通過技術手段自動調控，能夠最大限度的調動供給方的積極性，提升達成率</li><li>用戶瞭解了規則以後，會自發調節出行時間，避開高峰，從而服務雙方更合理平滑</li><li>實現自調度，降低了平臺的調度和維護成本，每一個個體都會為整個系統貢獻自己的力量，這也是失控理論的提現。</li></ol><p><span style=\"color: #ff6600;\"><strong>2、弊端</strong></span></p><ol><li>因為要付出更高的價格，部分用戶選擇不打車，而改用其他方式，這是平臺用戶的流失;</li><li>如果我們希望每位打車的人都能夠打到車，這個方案顯然是不能實現的，也就是說這並不是一個完美的方案。</li></ol><h2><strong>五、動態定價策略適用案例</strong></h2><p>基於動態定價的策略，我們來分析一下幾個典型的案例和實用場景：</p><p><span style=\"color: #ff6600;\"><strong>1、共享單車</strong></span></p><p>目前很火的共享單車們，跟共享經濟還是有很大的差異性，目前來看，多數共享單車是自己購買車輛，部分幾乎很少的車輛為C端用戶共享的車輛，所以這類完全是重資產的 B2C 企業，與我們討論的內容不符，只是借用了一個共享的冠名而已。</p><p><span style=\"color: #ff6600;\"><strong>2、快遞跑腿服務平臺</strong></span></p><p>如人人快遞，校內達和達達等，這類供需雙方都是普通的C端用戶，這一類的共享經濟體都可以採取動態定價的策略，以校內達為例，先以一個合理定價在 3 元左右的客單價，然後基於單個學校的小範圍內供需關係調整價格，從而也實現雙方的平衡。</p><p><span style=\"color: #ff6600;\"><strong>3、任務眾包平臺</strong></span></p><p>如豬八戒，碼市等，這類目前多採取的是競價模式，其實在競價模式的基礎上，也是可以引入動態定價策略，但是這類平臺考量的因素要比較多，比如團隊的經驗，能力等這些可能在價格方面占據的比重會更加大。</p><h2><strong>六、結語</strong></h2><p>動態定價策略是技術對經濟學的體現，也是基於實際問題思考而得出的解決方案，基於這一個動態的圓形，可以不斷加入完善策略，使它更加具有引導性，也使得市場更加合理。</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/77565\">10分鐘讀懂Uber的動態定價策略</a></p> </article>','2017-04-17','news',NULL,NULL,'http://www.36dsj.com/archives/77565',3,'本文所指的動態定價策略僅僅限定在共享經濟中的討論，其他類型中的暫不涵蓋。',NULL),
	(10,'TensorFlow 1.0 發佈，更快、更靈活、更方便開發','vivian','<article class=\"article-content\"><p><img alt=\"大數據\" class=\"36img\" src=\"../static/pic/learn/10_1.jpg\"/></p><p>在Mountain View舉辦的首屆年度TensorFlow開發者峰會上，Google 發佈了 TensorFlow 1.0發佈，保證了 Google 的機器學習庫的API穩定性。</p><p>發佈會視頻：https://youtu.be/LqLyrl-agOw</p><p><strong>主要亮點如下：</strong></p><p><strong>更快</strong>：TensorFlow 1.0 現在簡直快到難以置信! XLA為未來更多的性能改進奠定了基礎，而現在 tensorflow.org 調整了模型以實現最大速度。很快我們將發佈幾個流行模型，以展示如何充分利用TensorFlow 1.0 – 包括針對Inception v3的8位 GPU的7.3x加速和針對64位 GPU的分佈式Inception v3的58x加速!</p><p><strong>更靈活</strong>：TensorFlow 1.0引入了一個高級API，包含tf.layers，tf.metrics和tf.losses模塊。同時還宣佈增加了一個新的tf.keras模塊，它與另一個流行的高級神經網絡庫Keras完全兼容。</p><p><strong>更便於開發</strong>：TensorFlow 1.0 保證了 Python API穩定性(查看細節)，可以不破壞現有的代碼便能獲取新功能</p><p><strong>TensorFlow 1.0的其他亮點：</strong></p><ol><li>現在的 Python API 更類似於NumPy。此類和其他向後兼容的更改主要用來支持API穩定性，未來，我們將提供方便的遷移指南和轉換腳本。</li><li>Java和Go的Experimental API</li><li>更高級別的API模塊tf.layers，tf.metrics和tf.losses – 在合併skflow和TF Slim之後從tf.contrib.learn中提取</li><li>發佈面向CPU和GPU的TensorFlow圖表的領域特定編譯器 XLA的實驗版。 XLA正在迅速發展，預計在未來的發佈會中將看到更多的進展。</li><li>介紹一個用於調試實時TensorFlow程序的命令行界面和API – TensorFlow Debugger(tfdbg)。</li><li>用於對象檢測和本地化的新Android演示以及基於攝像頭的圖片樣式化。</li><li>安裝改進：添加了Python 3 docker鏡像，TensorFlow的pip包現在是PyPI兼容。這意味著TensorFlow現在可以安裝與pip安裝張量流的簡單調用。</li></ol><p>End.</p><p> </p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/77612\">TensorFlow 1.0 發佈，更快、更靈活、更方便開發</a></p> </article>','2017-04-17','learn',NULL,NULL,'http://www.36dsj.com/archives/77612',1,'在Mountain View舉辦的首屆年度TensorFlow開發者峰會上，Google 發佈了 TensorFlow 1.0發佈，保證了 Google 的機器學習庫的API穩定性。',NULL);

/*!40000 ALTER TABLE `article` ENABLE KEYS */;
UNLOCK TABLES;


# Dump of table user
# ------------------------------------------------------------

LOCK TABLES `user` WRITE;
/*!40000 ALTER TABLE `user` DISABLE KEYS */;

INSERT INTO `user` (`id`, `username`, `password_hash`)
VALUES
	(1,'admin','pbkdf2:sha1:1000$viSKWrbe$a802f2762f90f69493f70eb7bb8e57eb6d458eee');

/*!40000 ALTER TABLE `user` ENABLE KEYS */;
UNLOCK TABLES;



/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
