{% extends "post_base.html" %}
    {% block title %}Big Data On code{% endblock %}
    {% block detail %}
    <h1>如何用Spark解決一些經典MapReduce問題？</h1>
<p><img alt="screenshot" src="{{url_for('static',filename='pic/code_1_1.jpg')}}" /></p>
<p>文 | 譚楊</p>

<h3><strong>摘要</strong></h3>

<p>Spark是一個Apache項目，它被標榜為“快如閃電的集群計算”。它擁有一個繁榮的開源社區，並且是目前最活躍的Apache項目。 Spark提供了一個更快、更通用的數據處理平台。和Hadoop相比，Spark可以讓你的程序在內存中運行時速度提升100倍，或者在磁盤上運行時速度提升10倍。同時spark也讓傳統的map reduce job開髮變得更加簡單快捷。本文將簡單介紹幾個經典hadoop的mr按理用spark實現，來讓大家熟悉spark的開發。 </p>

<h3><strong>最大值最小值</strong></h3>

<p>求最大值最小值一直是Hadoop的經典案例，我們用Spark來實現一下，藉此感受一下spark中mr的思想和實現方式。話不多說直接上code：</p>

<p><img alt="screenshot" src="{{url_for('static',filename='pic/code_1_2.jpg')}}" /></p>
<p>&nbsp;</p>

<p>預期結果：</p>

<p>max: 1001min: 2</p>

<p>思路和hadoop中的mr類似，設定一個key，value為需要求最大與最小值的集合，然後再groupBykey聚合在一起處理。第二個方法就更簡單，性能也更好。 </p>

<h3><strong>平均值問題</strong></h3>

<p>求每個key對應的平均值是常見的案例，在spark中處理類似問題常常會用到combineByKey這個函數，詳細介紹請google一下用法，下面看代碼：</p>

<p><img alt="screenshot" src="{{url_for('static',filename='pic/code_1_3.jpg')}}" /></p>
<p>&nbsp;</p>

<p>我們讓每個partiton先求出單個partition內各個key對應的所有整數的和sum以及個數count，然後返回一個pair(sum, count)在shuffle後累加各個key對應的所有sum和count,再相除得到均值.</p>

<h3><strong>TopN問題</strong></h3>

<p>Top n問題同樣也是hadoop種體現mr思想的經典案例,那麼在spark中如何方便快捷的解決呢：</p>

<p><img alt="screenshot" src="{{url_for('static',filename='pic/code_1_4.jpg')}}" />
<p>&nbsp;</p>
<p>思路很簡單，把數據groupBykey以後按key形成分組然後取每個分組最大的2個。預期結果：</p></p>
<p><img alt="screenshot" src="{{url_for('static',filename='pic/code_1_5.jpg')}}" /></p>
<p>&nbsp;</p>

<p>以上簡單介紹了一下hadoop中常見的3個案例在spark中的實現。如果讀者們已經接觸過或者寫過一些hadoop的mapreduce job，那麼會不會覺得在spark中寫起來方便快捷很多呢。 </p>

<p>更多spark經典案例介紹期待下回分解。 。 。 </p>

<p>End.</p>

<p>轉載請註明來自36大數據（36dsj.com)：<a href="http://www.36dsj.com">36大數據</a> &raquo; <a href="http:/ /www.36dsj.com/archives/70751">如何用Spark解決一些經典MapReduce問題？ </a></p>
    {% endblock %}