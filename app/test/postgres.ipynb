{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgres://toppqsvvpcgceh:8e560b0767dc6b30d5140589013a2baca8752165a642c8188459ca806176fa4c@ec2-54-243-252-91.compute-1.amazonaws.com:5432/da1ftjs8usjpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = engine.execute(\"SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(sql):\n",
    "    return engine.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pid', 'bigint', None)\n",
      "('title', 'character varying', 1000)\n",
      "('author', 'character varying', 128)\n",
      "('content', 'text', None)\n",
      "('scontent', 'text', None)\n",
      "('date', 'character varying', 128)\n",
      "('article_type', 'character varying', 128)\n",
      "('tag', 'character varying', 128)\n",
      "('view', 'integer', None)\n",
      "('url_from', 'character varying', 128)\n",
      "('pic_count', 'integer', None)\n",
      "('sort', 'integer', None)\n",
      "('mkcontent', 'text', None)\n",
      "('mode', 'character varying', 20)\n"
     ]
    }
   ],
   "source": [
    "result = engine.execute(\"select column_name, data_type, character_maximum_length from INFORMATION_SCHEMA.COLUMNS where table_name = 'article';\")\n",
    "\n",
    "for a in result:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = engine.execute('ALTER TABLE article ADD COLUMN mode character varying(20);')\n",
    "result = engine.execute('SELECT article.pid AS article_pid, article.title AS article_title, article.mode AS article_mode, article.author AS article_author, article.content AS article_content, article.mkcontent AS article_mkcontent, article.scontent AS article_scontent, article.date AS article_date, article.article_type AS article_article_type, article.tag AS article_tag, article.view AS article_view, article.url_from AS article_url_from, article.pic_count AS article_pic_count, article.sort AS article_sort FROM article ORDER BY article.pid DESC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, '科普Spark，Spark是什麼，如何使用Spark | BIN 大數據', None, '<article class=\"article-content\"><p>本文章可以解答以下問題：</p><p>1.Spark基於什麼算法的分佈式計算（很簡單）<br/>2.Spark與MapReduce不同在什麼地方<br/>3.Spark為什麼比Hadoop靈活<br/>4.Spark局限是什麼<br/>5.什麼情況下適合使用Spark</p><p><img alt=\"spark \" class=\"36img\" src=\"../static/pic/6_1.jpg\"/></p><p><span style=\"color: #ff6600;\"><strong>什麼是Spark</strong></span></p><p>Spark是UC Berkeley AMP lab所開源的類Hadoop MapReduce的通用的並行計算框架，Spark基於map reduce算法實現的分佈式計算，擁有Hadoop MapReduce所具有的優點；但不同於MapReduce的是Job中間輸出和結果可以保存在內存中，從而不再需要讀寫HDFS，因此Spark能更好地適用於數據挖掘與機器學習等需要迭代的map reduce的算法。其架構如下圖所示：</p><p><a href=\"http://www.36dsj.com/wp-content/uploads/2014/11/3214.jpg\"><img alt=\"spark 大數據\" class=\"36img\" src=\"../static/pic/6_2.jpg\"/></a><br/><strong><span style=\"color: #ff6600;\">Spark與Hadoop的對比</span></strong></p><p>Spark的中間數據放到內存中，對於迭代運算效率更高。</p><p>Spark更適合於迭代運算比較多的ML和DM運算。因為在Spark裡面，有RDD的抽象概念。</p><p><strong><span style=\"color: #ff6600;\">Spark比Hadoop更通用</span></strong></p><p>Spark提供的數據集操作類型有很多種，不像Hadoop只提供了Map和Reduce兩種操作。比如map, filter, flatMap, sample, groupByKey, reduceByKey, union, join, cogroup, mapValues, sort,partionBy等多種操作類型，Spark把這些操作稱為Transformations。同時還提供Count, collect, reduce, lookup, save等多種actions操作。<br/>這些多種多樣的數據集操作類型，給給開發上層應用的用戶提供了方便。各個處理節點之間的通信模型不再像Hadoop那樣就是唯一的Data Shuffle一種模式。用戶可以命名，物化，控制中間結果的存儲、分區等。可以說編程模型比Hadoop更靈活。<br/>不過由於RDD的特性，Spark不適用那種異步細粒度更新狀態的應用，例如web服務的存儲或者是增量的web爬蟲和索引。就是對於那種增量修改的應用模型不適合。</p><p><span style=\"color: #ff6600;\"><strong>容錯性</strong></span></p><p>在分佈式數據集計算時通過checkpoint來實現容錯，而checkpoint有兩種方式，一個是checkpoint data，一個是logging the updates。用戶可以控制採用哪種方式來實現容錯。</p><p><strong><span style=\"color: #ff6600;\">可用性</span></strong></p><p>Spark通過提供豐富的Scala, Java，Python API及交互式Shell來提高可用性。</p><p><strong><span style=\"color: #ff6600;\">Spark與Hadoop的結合</span></strong></p><p>Spark可以直接對HDFS進行數據的讀寫，同樣支持Spark on YARN。Spark可以與MapReduce運行於同集群中，共享存儲資源與計算，數據倉庫Shark實現上借用Hive，幾乎與Hive完全兼容。</p><p><strong><span style=\"color: #ff6600;\">Spark的適用場景</span></strong></p><p>Spark是基於內存的迭代計算框架，適用於需要多次操作特定數據集的應用場合。需要反覆操作的次數越多，所需讀取的數據量越大，受益越大，數據量小但是計算密集度較大的場合，受益就相對較小（大數據庫架構中這是是否考慮使用Spark的重要因素）</p><p>由於RDD的特性，Spark不適用那種異步細粒度更新狀態的應用，例如web服務的存儲或者是增量的web爬蟲和索引。就是對於那種增量修改的應用模型不適合。<br/>總的來說Spark的適用面比較廣泛且比較通用。</p><p><span style=\"color: #ff6600;\"><strong>運行模式</strong></span></p><ul><li>本地模式</li><li>Standalone模式</li><li>Mesoes模式</li><li>yarn模式</li></ul><p><strong><span style=\"color: #ff6600;\">Spark生態系統</span></strong></p><p>Shark ( Hive on Spark): Shark基本上就是在Spark的框架基礎上提供和Hive一樣的H iveQL命令接口，為了最大程度的保持和Hive的兼容性，Shark使用了Hive的API來實現query Parsing和 Logic Plan generation，最後的PhysicalPlan execution階段用Spark代替Hadoop MapReduce。通過配置Shark參數，Shark可以自動在內存中緩存特定的RDD，實現數據重用，進而加快特定數據集的檢索。同時，Shark通過UDF用戶自定義函數實現特定的數據分析學習算法，使得SQL數據查詢和運算分析能結合在一起，最大化RDD的重覆使用。</p><p>Spark streaming: 構建在Spark上處理Stream數據的框架，基本的原理是將Stream數據分成小的時間片斷（幾秒），以類似batch批量處理的方式來處理這小部分數據。Spark Streaming構建在Spark上，一方面是因為Spark的低延遲執行引擎（100ms+）可以用於實時計算，另一方面相比基於Record的其它處理框架（如Storm），RDD數據集更容易做高效的容錯處理。此外小批量處理的方式使得它可以同時兼容批量和實時數據處理的邏輯和算法。方便了一些需要歷史數據和實時數據聯合分析的特定應用場合。</p><p>Bagel: Pregel on Spark，可以用Spark進行圖計算，這是個非常有用的小項目。Bagel自帶了一個例子，實現了Google的PageRank算法。</p><p>End.</p><p>轉載請註明來自36大數據（36dsj.com)：<a href=\"http://www.36dsj.com\">36大數據</a> » <a href=\"http://www.36dsj.com/archives/17187\">科普Spark，Spark是什麼，如何使用Spark</a></p> </article>', 'Spark是UC Berkeley AMP lab所開源的類Hadoop MapReduce的通用的並行計算框架，Spark基於map reduce算法實現的分佈式計算，擁有Hadoop MapReduce所具有的優點；但不同於MapReduce的是Job中間輸出和結果可以保存在內存中，從而不再需要讀寫HDFS，因此Spark能更好地適用於數據挖掘與機器學習等需要迭代的map reduce的算法。其架構如下圖所示：', None, 'code', None, None, 'http://www.36dsj.com/archives/17187', None, None, None, 'crawler')\n"
     ]
    }
   ],
   "source": [
    "sql = 'select max(pid) from article'\n",
    "sql = 'DELETE FROM article WHERE pid = 6;'\n",
    "sql = 'select * from article where pid = 6'\n",
    "\n",
    "result = search(sql)\n",
    "\n",
    "for a in result:\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'INSERT INTO films (pid ,article_type ,url_from ,mode ,title ,content ,scontent ) VALUES (%s,%s,%s,%s,%s,%s,%s);'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
